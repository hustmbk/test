import os
import sys
import json
import math
import torch
import argparse
import random
import numpy as np
from termcolor import colored
from transformers import AutoTokenizer

# [ä¿®æ­£] ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•è¢«æ­£ç¡®æ·»åŠ åˆ°è·¯å¾„ä¸­
# è¿™æ ·å¯ä»¥æ‰¾åˆ° model_hub ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

# [å®˜æ–¹åšæ³•] ä» model_hub å¯¼å…¥é‡æ„åçš„æ¨¡å‹ç±»
# æˆ‘ä»¬å‡è®¾ deepseek_v2_model.py å’Œå…¶ä»–æ¨¡å‹æ–‡ä»¶éƒ½åœ¨ model_hub ç›®å½•ä¸‹
try:
    from model_hub import DeepSeekV2Model
    from model_hub import LlamaModel, QwenModel # å¦‚æœéœ€è¦ï¼Œå–æ¶ˆæ³¨é‡Š
except ImportError:
    print(colored("é”™è¯¯: æ— æ³•ä» 'model_hub' å¯¼å…¥æ¨¡å‹ã€‚è¯·ç¡®ä¿æ‚¨çš„æ–‡ä»¶ç»“æ„å¦‚ä¸‹:", 'red'))
    print("your_project/")
    print("â”œâ”€â”€ test.py (æ­¤æ–‡ä»¶)")
    print("â””â”€â”€ model_hub/")
    print("    â”œâ”€â”€ __init__.py")
    print("    â””â”€â”€ deepseek_v2_model.py")
    sys.exit(1)


def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def parse_args():
    parser = argparse.ArgumentParser(description="Test example with official DeepSeek-V2 MLA support")
    parser.add_argument("--batch_size", type=int, default=1, help="Batch size")
    parser.add_argument("--gen_len", type=int, default=100, help="Generation length")
    parser.add_argument("--device", type=str, default="auto", help="Device map ('auto', 'cuda:0', etc.)")
    parser.add_argument("--dtype", type=str, default="bf16", choices=["fp16", "bf16"], help="Dtype")
    
    # [ä¿®æ­£] é»˜è®¤ä½¿ç”¨ Full_Flash_Attnï¼Œå› ä¸ºè¿™æ˜¯é’ˆå¯¹DeepSeek-V2çš„å®˜æ–¹æ¨èåšæ³•
    parser.add_argument("--attn_type", type=str, default="Full_Flash_Attn",
                        choices=["Full_Flash_Attn", "RetroInfer"], help="Attention method")
    
    parser.add_argument("--model_name", type=str, default="deepseek-ai/DeepSeek-V2-Lite-Chat",
                        help="HuggingFace model name or local path")
    parser.add_argument("--model_type", type=str, default="auto",
                        choices=["auto", "llama", "qwen", "deepseek"],
                        help="Force model type (auto-detect if 'auto')")
    
    parser.add_argument("--data_path", type=str, default="", help="Input json file path")
    parser.add_argument("--test_mla_fix", action="store_true", help="Run diagnostic test for MLA architecture.")
    
    args = parser.parse_args()
    return args


def detect_model_type(model_name):
    """Auto-detect model type from model name"""
    model_name_lower = model_name.lower()
    if any(keyword in model_name_lower for keyword in ['deepseek', 'deepseek-v2', 'deepseek-coder']):
        return 'deepseek'
    elif any(keyword in model_name_lower for keyword in ['llama', 'llama-3', 'llama-2']):
        return 'llama'
    elif any(keyword in model_name_lower for keyword in ['qwen', 'qwen2']):
        return 'qwen'
    else:
        print(colored(f"âš ï¸  Cannot auto-detect model type for {model_name}, assuming Llama.", 'yellow'))
        return 'llama'

def generate_config(attn_type, context_len):
    """
    [ä¿®æ­£] ç®€åŒ–é…ç½®ç”Ÿæˆã€‚
    å¯¹äº Full_Flash_Attnï¼Œä¸éœ€è¦ä»»ä½•ç‰¹æ®Šé…ç½®ã€‚
    å¯¹äº RetroInferï¼Œä¿ç•™å…¶è®¡ç®—é€»è¾‘ã€‚
    """
    if attn_type == "Full_Flash_Attn":
        print(colored("Using Full_Flash_Attn: No special config needed.", 'cyan'))
        return None

    # RetroInferçš„é…ç½®é€»è¾‘ä¿æŒä¸å˜
    if attn_type == 'RetroInfer':
        print(colored(f"Generating config for RetroInfer with context length {context_len}...", 'cyan'))
        n_clusters = max(int(context_len / 16), 1)
        n_segments = max(int(context_len / 8192), 1)
        segment_multiple = n_segments * 32
        if segment_multiple > 0:
            lower = (n_clusters // segment_multiple) * segment_multiple
            upper = lower + segment_multiple
            n_clusters = lower if abs(n_clusters - lower) <= abs(n_clusters - upper) else upper
        nprobe = max(int(n_clusters * 0.018), 1)
        
        config = {
            "RetroInfer": {
                "n_centroids": n_clusters,
                "n_segment": n_segments,
                "nprobe": nprobe,
                "cache_cluster_num": nprobe * 3,
                "max_compute_cluster_num": max(int(n_clusters / 4), nprobe)
            }
        }
        print(json.dumps(config[attn_type], indent=2))
        return config
    
    return None


def test_mla_architecture(llm):
    """[ä¿®æ­£] åŸºäºæ¨¡å‹åŠ è½½åçš„configå¯¹è±¡æ¥è¯Šæ–­MLAæ¶æ„ï¼Œæ›´ç¨³å®šå¯é ã€‚"""
    if not isinstance(llm, DeepSeekV2Model):
        print(colored("âš ï¸  Not a DeepSeekV2Model instance, skipping MLA test.", 'yellow'))
        return

    print(colored("\nğŸ§ª Running MLA Architecture Sanity Check:", 'cyan'))
    config = llm.config

    try:
        # æµ‹è¯•1ï¼šéªŒè¯KVå¤´æ•°ï¼ˆMLAä¸­åº”ç­‰äºQå¤´æ•°ï¼‰
        if config.num_key_value_heads == config.num_attention_heads:
            print(colored(f"âœ… Test 1 Passed: num_key_value_heads ({config.num_key_value_heads}) == num_attention_heads ({config.num_attention_heads})", 'green'))
        else:
            print(colored(f"âŒ Test 1 Failed: KV heads ({config.num_key_value_heads}) != Q heads ({config.num_attention_heads})", 'red'))

        # æµ‹è¯•2ï¼šéªŒè¯ç»´åº¦ï¼ˆä¸åŒçš„Q/Kå’ŒVç»´åº¦æ˜¯MLAçš„å…³é”®ç‰¹å¾ï¼‰
        q_head_dim = config.qk_nope_head_dim + config.qk_rope_head_dim
        v_head_dim = config.v_head_dim
        if q_head_dim == 192 and v_head_dim == 128:
            print(colored(f"âœ… Test 2 Passed: Q/K head dim is {q_head_dim}D, V head dim is {v_head_dim}D.", 'green'))
        else:
            print(colored(f"âŒ Test 2 Failed: Unexpected dimensions Q/K={q_head_dim}D, V={v_head_dim}D.", 'red'))

        # æµ‹è¯•3ï¼šéªŒè¯KV LoRA Rankå­˜åœ¨
        if hasattr(config, 'kv_lora_rank') and config.kv_lora_rank > 0:
            print(colored(f"âœ… Test 3 Passed: kv_lora_rank is present ({config.kv_lora_rank}).", 'green'))
        else:
            print(colored("âŒ Test 3 Failed: kv_lora_rank not found or is zero.", 'red'))

        print(colored("\nğŸ“‹ MLA Architecture Summary:", 'white'))
        print("  - MLA (Multi-head Latent Attention) uses low-rank projection to compress the KV cache.")
        print("  - It does NOT reduce the number of heads; it reduces the dimension of what's stored.")
        print("  - The result is a >90% reduction in KV cache memory with comparable performance.")
        
    except AttributeError as e:
        print(colored(f"âŒ Diagnostic test failed: A required config attribute is missing: {e}", 'red'))
    print()


def load_model(model_name, model_type, max_len, dtype, device):
    """åŠ è½½æ¨¡å‹ï¼Œæ ¹æ®ç±»å‹é€‰æ‹©æ­£ç¡®çš„ç±»ã€‚"""
    if model_type == "auto":
        model_type = detect_model_type(model_name)
    
    print(colored(f"Loading model of type '{model_type}': {model_name}", 'cyan'))
    
    # [å®˜æ–¹åšæ³•] å½“æ¨¡å‹æ˜¯deepseekæ—¶ï¼Œå®ä¾‹åŒ–æˆ‘ä»¬é‡æ„åçš„DeepSeekV2Model
    if model_type == 'deepseek':
        return DeepSeekV2Model(
            model_name,
            max_length=max_len,
            dtype=dtype,
            device_map=device
        )
    # elif model_type == 'llama':
    #     return LlamaModel(...)
    # elif model_type == 'qwen':
    #     return QwenModel(...)
    else:
        raise ValueError(f"Unsupported model type: {model_type}. Please implement or choose another model.")


def main():
    args = parse_args()
    set_seed(2025)

    dtype = torch.float16 if args.dtype == 'fp16' else torch.bfloat16

    print(colored("="*50, 'cyan'))
    print(colored("ğŸš€ Starting DeepSeek-V2 Test (Official Implementation)", 'cyan'))
    print(f"   Model: {args.model_name}")
    print(f"   Attention: {args.attn_type}")
    print(f"   Batch Size: {args.batch_size}")
    print(colored("="*50, 'cyan'))

    # åŠ è½½æ¨¡å‹
    max_len = 2048 # è®¾å®šä¸€ä¸ªåˆç†çš„é»˜è®¤æœ€å¤§é•¿åº¦
    llm = load_model(args.model_name, args.model_type, max_len, dtype, args.device)
    
    # å¦‚æœéœ€è¦ï¼Œè¿è¡Œæ¶æ„è¯Šæ–­
    if args.test_mla_fix and args.model_type in ['deepseek', 'auto']:
        test_mla_architecture(llm)

    # åŠ è½½å¹¶å‡†å¤‡æ•°æ®
    if not args.data_path:
        print(colored("No data path provided, using default prompt.", 'yellow'))
        prompts = ["Write a short story about a robot who discovers music."] * args.batch_size
    else:
        try:
            with open(args.data_path, 'r') as f:
                data = json.load(f)
            prompts = [item['input'] for item in data] * math.ceil(args.batch_size / len(data))
            prompts = prompts[:args.batch_size]
        except Exception as e:
            print(colored(f"Error loading data file: {e}. Using default prompt.", 'red'))
            prompts = ["Hello, how are you?"] * args.batch_size

    # [ä¿®æ­£] å¯¹äºFull_Flash_Attnæ¨¡å¼ï¼Œä¸å†éœ€è¦å¤æ‚çš„attn_config
    # generate_configå‡½æ•°ç°åœ¨åªä¸ºRetroInferè¿”å›æœ‰æ•ˆé…ç½®
    attn_config = generate_config(args.attn_type, 1024) # ä¼ å…¥ä¸€ä¸ªç¤ºä¾‹é•¿åº¦

    # --- æ‰§è¡Œæ¨ç† ---
    # [å®˜æ–¹åšæ³•] è°ƒç”¨æ¨¡å‹ä¸Šæ›´ç®€æ´ã€é¢å‘ç”¨æˆ·çš„generateæ–¹æ³•ã€‚
    # æ¨¡å‹å†…éƒ¨ä¼šå¤„ç†åˆ†è¯ã€ç¼“å­˜è®¾ç½®å’Œå®Œæ•´çš„prefill/decodeå¾ªç¯ã€‚
    for i, prompt in enumerate(prompts):
        print(colored(f"\n--- Generating for Sample {i+1}/{args.batch_size} ---", 'yellow'))
        print(f"Prompt: {prompt}")
        
        try:
            # llm.generateç°åœ¨æ˜¯ä¸»è¦çš„æ¨ç†å…¥å£
            response = llm.generate(
                inputs=prompt,
                max_new_tokens=args.gen_len
            )
            print(colored("Generated Response:", 'green'))
            print(response)
        except Exception as e:
            print(colored(f"âŒ Generation failed for sample {i+1}: {e}", 'red'))
            import traceback
            traceback.print_exc()

    print(colored("\nğŸ‰ Test completed!", 'cyan'))


if __name__ == "__main__":
    main()
